#!/usr/bin/env python3
"""
KAI çŸ¥è¯†åº“é—®ç­”è„šæœ¬ - ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³å†…å®¹å¹¶ç”Ÿæˆå›ç­”

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 ask_kai.py "ä½ çš„é—®é¢˜"

ä¾èµ–ï¼š
    - langchain
    - langchain-community
    - chromadb
    - langchain-huggingface
    - FlagEmbedding (ç”¨äº Rerank V3.3)

V3.3 å‡çº§ï¼šå¼•å…¥ Rerank é‡æ’åºæ¨¡å—
- Top-K (20) -> Rerank (ç²¾æ’) -> Top-N (5)
- ä½¿ç”¨ BAAI/bge-reranker-v2-m3 æ¨¡å‹
"""

import os
import sys
import logging
from datetime import datetime
from dotenv import load_dotenv

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ========== é…ç½® ==========
PERSIST_DIR = "./chroma_db_data"
OUTPUT_DIR = "./outputs"

# Rerank é…ç½® (V3.3)
RERANK_ENABLED = True
RERANK_MODEL = "BAAI/bge-reranker-v2-m3"
RERANK_TOP_K = 20    # ç²—æ’ï¼šå‘é‡æ£€ç´¢è¿”å›å€™é€‰æ•°
RERANK_TOP_N = 5     # ç²¾æ’ï¼šRerank åè¿”å›æœ€ç»ˆæ•°

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆç”¨äº LLM APIï¼‰
load_dotenv()

# ========== æ—¥å¿—é…ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def save_output(question, answer, sources):
    """
    ä¿å­˜é—®ç­”ç»“æœåˆ° Markdown æ–‡ä»¶

    æ–‡ä»¶åæ ¼å¼ï¼šæ—¶é—´æˆ³_é—®é¢˜å‰10å­—.md
    ä¾‹å¦‚ï¼š20260106_1200_å¦‚ä½•æ„å»ºä¿¡ä»».md
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    # æ¸…ç†é—®é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œå–å‰10ä¸ªå­—
    safe_title = "".join(c for c in question if c.isalnum() or c in "_-â€”â€” ")[:10].strip()
    safe_title = safe_title.replace(" ", "")
    filename = f"{timestamp}_{safe_title}.md"
    filepath = os.path.join(OUTPUT_DIR, filename)

    # æ„å»º Markdown å†…å®¹
    md_content = f"""# {question}

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M")}

## é—®é¢˜

{question}

## å›ç­”

{answer}

## å¼•ç”¨æ¥æº

"""

    for i, source in enumerate(sources, 1):
        md_content += f"- {source}\n"

    # å†™å…¥æ–‡ä»¶
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_content)

    return filepath


def get_embedding_model():
    """
    è·å– embedding æ¨¡å‹

    âš ï¸ é‡è¦ï¼šå¿…é¡»ä¸ build_index.py ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼
    è¿™é‡Œä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡æ¨¡å‹ï¼Œä¸å†™å…¥ç«¯ä¿æŒä¸€è‡´ã€‚
    """
    try:
        from langchain_huggingface import HuggingFaceEmbeddings

        logger.info("ä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = HuggingFaceEmbeddings(
            model_name="shibing624/text2vec-base-chinese",
            model_kwargs={'device': 'cpu'}
        )
        return embeddings
    except ImportError:
        # Fallback to sentence-transformers
        from langchain_community.embeddings import SentenceTransformerEmbeddings

        logger.info("ä½¿ç”¨ SentenceTransformer ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = SentenceTransformerEmbeddings(
            model_name="shibing624/text2vec-base-chinese"
        )
        return embeddings


def get_llm():
    """
    è·å– LLM æ¨¡å‹ç”¨äºç”Ÿæˆå›ç­”

    æ”¯æŒï¼š
    - MiniMax (abab6.5s-chat)
    - DeepSeek
    - OpenAI å…¼å®¹ API
    """
    api_base = os.getenv("OPENAI_API_BASE", "").rstrip("/")
    api_key = os.getenv("OPENAI_API_KEY", "")
    chat_model = os.getenv("CHAT_MODEL", "abab6.5s-chat")

    # å¦‚æœæœ‰é…ç½® APIï¼Œä½¿ç”¨åœ¨çº¿ LLM
    if api_base and api_key:
        logger.info(f"ä½¿ç”¨åœ¨çº¿ LLM: {chat_model}")
        llm = ChatOpenAI(
            model=chat_model,
            openai_api_base=api_base,
            openai_api_key=api_key,
            temperature=0.7
        )
    else:
        # Fallback: ç®€å•è§„åˆ™åŒ¹é…ï¼ˆæ—  API æ—¶ä½¿ç”¨ï¼‰
        logger.warning("æœªé…ç½® LLM APIï¼Œå°†ä½¿ç”¨åŸºäºè§„åˆ™çš„å›ç­”")
        from langchain.llms import FakeListLLM
        llm = FakeListLLM(responses=["è¯·é…ç½® LLM API ä»¥è·å¾—æ™ºèƒ½å›ç­”"])

    return llm


# Rerank æ¨¡å‹ç¼“å­˜ï¼ˆå…¨å±€å˜é‡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
_reranker_model = None


def get_reranker():
    """
    è·å– Rerank æ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    V3.3 æ–°å¢ï¼šä½¿ç”¨ BAAI/bge-reranker-v2-m3 è¿›è¡Œç²¾æ’
    """
    global _reranker_model

    if _reranker_model is None:
        try:
            from FlagEmbedding import FlagReranker
            logger.info(f"åŠ è½½ Rerank æ¨¡å‹: {RERANK_MODEL}")
            _reranker_model = FlagReranker(RERANK_MODEL, use_fp16=True)
            logger.info("âœ… Rerank æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âŒ Rerank æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            _reranker_model = None

    return _reranker_model


def rerank_docs(query, documents):
    """
    V3.3 æ–°å¢ï¼šä½¿ç”¨ Rerank æ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œç²¾æ’

    æµç¨‹ï¼šTop-K (20) -> Rerank -> Top-N (5)

    Args:
        query: ç”¨æˆ·é—®é¢˜
        documents: ç²—æ’å€™é€‰æ–‡æ¡£åˆ—è¡¨

    Returns:
        ç²¾æ’åçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆTop-Nï¼‰
    """
    if not RERANK_ENABLED:
        logger.info("Rerank æœªå¯ç”¨ï¼Œç›´æ¥è¿”å›ç²—æ’ç»“æœ")
        return documents[:RERANK_TOP_N]

    if len(documents) <= RERANK_TOP_N:
        logger.info(f"å€™é€‰æ–‡æ¡£æ•°é‡ ({len(documents)}) <= {RERANK_TOP_N}ï¼Œæ— éœ€ Rerank")
        return documents

    # åŠ è½½ Rerank æ¨¡å‹
    reranker = get_reranker()
    if reranker is None:
        logger.warning("Rerank æ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›ç²—æ’ç»“æœ")
        return documents[:RERANK_TOP_N]

    logger.info(f"ğŸ”„ Rerank ç²¾æ’ä¸­: {len(documents)} -> {RERANK_TOP_N}")

    try:
        # æ„é€ é…å¯¹ï¼š[[query, doc1], [query, doc2], ...]
        pairs = [[query, doc.page_content] for doc in documents]

        # è®¡ç®—åˆ†æ•°
        scores = reranker.compute_score(pairs, normalize=True)

        # åˆå¹¶æ–‡æ¡£å’Œåˆ†æ•°
        combined = list(zip(documents, scores))

        # æŒ‰åˆ†æ•°é™åºæ’åºï¼ˆåˆ†æ•°é«˜çš„åœ¨å‰ï¼‰
        combined.sort(key=lambda x: x[1], reverse=True)

        # å– Top-N
        reranked = [doc for doc, score in combined[:RERANK_TOP_N]]

        logger.info(f"âœ… Rerank å®Œæˆï¼ŒTop-{RERANK_TOP_N} åˆ†æ•°èŒƒå›´: {combined[0][1]:.4f} - {combined[RERANK_TOP_N-1][1]:.4f}")

        return reranked

    except Exception as e:
        logger.warning(f"Rerank è®¡ç®—å¤±è´¥: {e}ï¼Œè¿”å›ç²—æ’ç»“æœ")
        return documents[:RERANK_TOP_N]


def create_qa_chain(vectorstore):
    """
    åˆ›å»ºé—®ç­”é“¾
    """
    llm = get_llm()

    # å®šåˆ¶ prompt
    prompt_template = """åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœå†…å®¹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´"çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹"ã€‚

=== èƒŒæ™¯çŸ¥è¯† ===
{context}

=== é—®é¢˜ ===
{question}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # åˆ›å»ºæ£€ç´¢é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": PROMPT}
    )

    return qa_chain


def main():
    """
    ä¸»æµç¨‹ï¼šåŠ è½½å‘é‡åº“ -> æ£€ç´¢ç›¸å…³å†…å®¹ -> ç”Ÿæˆå›ç­”
    """
    if len(sys.argv) < 2:
        print("âŒ è¯·æä¾›é—®é¢˜ï¼Œä¾‹å¦‚ï¼š")
        print("   python3 ask_kai.py \"å¦‚ä½•æå‡ä¸ªäººèƒ½åŠ›ï¼Ÿ\"")
        return

    question = sys.argv[1]

    print("=" * 60)
    print("KAI çŸ¥è¯†åº“é—®ç­”")
    print("=" * 60)
    print(f"\né—®é¢˜ï¼š{question}\n")

    # 1. æ£€æŸ¥å‘é‡åº“
    if not os.path.exists(PERSIST_DIR):
        print("âŒ å‘é‡åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build_index.py")
        return

    logger.info(f"åŠ è½½å‘é‡åº“: {PERSIST_DIR}")

    # 2. åŠ è½½ embedding æ¨¡å‹
    logger.info("åˆå§‹åŒ– embedding æ¨¡å‹...")
    embeddings = get_embedding_model()

    # 3. åŠ è½½å‘é‡åº“
    logger.info("åŠ è½½å‘é‡æ•°æ®åº“...")
    vectorstore = Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings
    )

    # 4. æ£€æŸ¥æ•°æ®
    count = vectorstore._collection.count()
    logger.info(f"å‘é‡åº“ä¸­å…±æœ‰ {count} ä¸ªç‰‡æ®µ")
    print(f"ğŸ“š å·²ç´¢å¼• {count} ä¸ªçŸ¥è¯†ç‰‡æ®µ\n")

    # 5. æ£€ç´¢ç›¸å…³å†…å®¹ï¼ˆV3.3 Rerank æµç¨‹ï¼‰
    print("ğŸ” æ£€ç´¢ç›¸å…³çŸ¥è¯†...\n")

    # 5.1 ç²—æ’ï¼šä»å‘é‡åº“è·å–æ›´å¤šå€™é€‰ (Top-K)
    logger.info(f"ğŸ“Š ç²—æ’ï¼šå‘é‡æ£€ç´¢ Top-{RERANK_TOP_K}")
    raw_docs = vectorstore.similarity_search(question, k=RERANK_TOP_K)
    logger.info(f"   ç²—æ’å€™é€‰æ•°: {len(raw_docs)}")

    # 5.2 ç²¾æ’ï¼šRerank æ¨¡å‹æ‰“åˆ†æ’åº (Top-N)
    logger.info(f"ğŸ“Š ç²¾æ’ï¼šRerank æ¨¡å‹æ‰“åˆ†")
    docs = rerank_docs(question, raw_docs)
    logger.info(f"   ç²¾æ’ç»“æœæ•°: {len(docs)}")

    # æ”¶é›†æ¥æºåˆ—è¡¨ï¼ˆå»é‡ï¼‰
    sources = []
    seen = set()
    for doc in docs:
        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
        if source not in seen:
            seen.add(source)
            sources.append(source)

    print("ã€æ£€ç´¢ç»“æœã€‘")
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
        # æ˜¾ç¤º Header å…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        header_info = ""
        if doc.metadata.get('Header 1'):
            header_info = f" [{doc.metadata.get('Header 1')}"
            if doc.metadata.get('Header 2'):
                header_info += f" > {doc.metadata.get('Header 2')}"
            header_info += "]"
        print(f"\n--- ç»“æœ {i} (æ¥æº: {source}){header_info} ---")
        # æ˜¾ç¤ºå‰200å­—
        content = doc.page_content[:200].replace('\n', ' ')
        print(f"{content}...")

    # 6. ç”Ÿæˆå›ç­”ï¼ˆä½¿ç”¨ Rerank åçš„æ–‡æ¡£ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ¤– KAI å›ç­”ï¼š")
    print("=" * 60)

    # æ„é€ ä¸Šä¸‹æ–‡
    context = "\n\n".join([doc.page_content for doc in docs])

    # è·å– LLM å¹¶ç”Ÿæˆå›ç­”
    llm = get_llm()
    prompt_template = """åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœå†…å®¹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´"çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹"ã€‚

=== èƒŒæ™¯çŸ¥è¯† ===
{context}

=== é—®é¢˜ ===
{question}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""

    from langchain.prompts import PromptTemplate
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # ç”Ÿæˆå›ç­”
    response = llm.invoke(PROMPT.format(context=context, question=question))
    answer = response.content if hasattr(response, 'content') else str(response)
    print(answer)

    # 7. ä¿å­˜åˆ°æ–‡ä»¶
    filepath = save_output(question, answer, sources)
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {filepath}")


if __name__ == "__main__":
    main()
