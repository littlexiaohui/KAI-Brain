#!/usr/bin/env python3
"""
KAI çŸ¥è¯†åº“é—®ç­”è„šæœ¬ - ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³å†…å®¹å¹¶ç”Ÿæˆå›ç­”

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 ask_kai.py "ä½ çš„é—®é¢˜"

ä¾èµ–ï¼š
    - langchain
    - langchain-community
    - chromadb
    - langchain-huggingface
"""

import os
import sys
import logging
from datetime import datetime
from dotenv import load_dotenv

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ========== é…ç½® ==========
PERSIST_DIR = "./chroma_db_data"
OUTPUT_DIR = "./outputs"

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆç”¨äº LLM APIï¼‰
load_dotenv()

# ========== æ—¥å¿—é…ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def save_output(question, answer, sources):
    """
    ä¿å­˜é—®ç­”ç»“æœåˆ° Markdown æ–‡ä»¶

    æ–‡ä»¶åæ ¼å¼ï¼šæ—¶é—´æˆ³_é—®é¢˜å‰10å­—.md
    ä¾‹å¦‚ï¼š20260106_1200_å¦‚ä½•æ„å»ºä¿¡ä»».md
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    # æ¸…ç†é—®é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œå–å‰10ä¸ªå­—
    safe_title = "".join(c for c in question if c.isalnum() or c in "_-â€”â€” ")[:10].strip()
    safe_title = safe_title.replace(" ", "")
    filename = f"{timestamp}_{safe_title}.md"
    filepath = os.path.join(OUTPUT_DIR, filename)

    # æ„å»º Markdown å†…å®¹
    md_content = f"""# {question}

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M")}

## é—®é¢˜

{question}

## å›ç­”

{answer}

## å¼•ç”¨æ¥æº

"""

    for i, source in enumerate(sources, 1):
        md_content += f"- {source}\n"

    # å†™å…¥æ–‡ä»¶
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(md_content)

    return filepath


def get_embedding_model():
    """
    è·å– embedding æ¨¡å‹

    âš ï¸ é‡è¦ï¼šå¿…é¡»ä¸ build_index.py ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼
    è¿™é‡Œä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡æ¨¡å‹ï¼Œä¸å†™å…¥ç«¯ä¿æŒä¸€è‡´ã€‚
    """
    try:
        from langchain_huggingface import HuggingFaceEmbeddings

        logger.info("ä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = HuggingFaceEmbeddings(
            model_name="shibing624/text2vec-base-chinese",
            model_kwargs={'device': 'cpu'}
        )
        return embeddings
    except ImportError:
        # Fallback to sentence-transformers
        from langchain_community.embeddings import SentenceTransformerEmbeddings

        logger.info("ä½¿ç”¨ SentenceTransformer ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = SentenceTransformerEmbeddings(
            model_name="shibing624/text2vec-base-chinese"
        )
        return embeddings


def get_llm():
    """
    è·å– LLM æ¨¡å‹ç”¨äºç”Ÿæˆå›ç­”

    æ”¯æŒï¼š
    - MiniMax (abab6.5s-chat)
    - DeepSeek
    - OpenAI å…¼å®¹ API
    """
    api_base = os.getenv("OPENAI_API_BASE", "").rstrip("/")
    api_key = os.getenv("OPENAI_API_KEY", "")
    chat_model = os.getenv("CHAT_MODEL", "abab6.5s-chat")

    # å¦‚æœæœ‰é…ç½® APIï¼Œä½¿ç”¨åœ¨çº¿ LLM
    if api_base and api_key:
        logger.info(f"ä½¿ç”¨åœ¨çº¿ LLM: {chat_model}")
        llm = ChatOpenAI(
            model=chat_model,
            openai_api_base=api_base,
            openai_api_key=api_key,
            temperature=0.7
        )
    else:
        # Fallback: ç®€å•è§„åˆ™åŒ¹é…ï¼ˆæ—  API æ—¶ä½¿ç”¨ï¼‰
        logger.warning("æœªé…ç½® LLM APIï¼Œå°†ä½¿ç”¨åŸºäºè§„åˆ™çš„å›ç­”")
        from langchain.llms import FakeListLLM
        llm = FakeListLLM(responses=["è¯·é…ç½® LLM API ä»¥è·å¾—æ™ºèƒ½å›ç­”"])

    return llm


def create_qa_chain(vectorstore):
    """
    åˆ›å»ºé—®ç­”é“¾
    """
    llm = get_llm()

    # å®šåˆ¶ prompt
    prompt_template = """åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœå†…å®¹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´"çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹"ã€‚

=== èƒŒæ™¯çŸ¥è¯† ===
{context}

=== é—®é¢˜ ===
{question}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # åˆ›å»ºæ£€ç´¢é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": PROMPT}
    )

    return qa_chain


def main():
    """
    ä¸»æµç¨‹ï¼šåŠ è½½å‘é‡åº“ -> æ£€ç´¢ç›¸å…³å†…å®¹ -> ç”Ÿæˆå›ç­”
    """
    if len(sys.argv) < 2:
        print("âŒ è¯·æä¾›é—®é¢˜ï¼Œä¾‹å¦‚ï¼š")
        print("   python3 ask_kai.py \"å¦‚ä½•æå‡ä¸ªäººèƒ½åŠ›ï¼Ÿ\"")
        return

    question = sys.argv[1]

    print("=" * 60)
    print("KAI çŸ¥è¯†åº“é—®ç­”")
    print("=" * 60)
    print(f"\né—®é¢˜ï¼š{question}\n")

    # 1. æ£€æŸ¥å‘é‡åº“
    if not os.path.exists(PERSIST_DIR):
        print("âŒ å‘é‡åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build_index.py")
        return

    logger.info(f"åŠ è½½å‘é‡åº“: {PERSIST_DIR}")

    # 2. åŠ è½½ embedding æ¨¡å‹
    logger.info("åˆå§‹åŒ– embedding æ¨¡å‹...")
    embeddings = get_embedding_model()

    # 3. åŠ è½½å‘é‡åº“
    logger.info("åŠ è½½å‘é‡æ•°æ®åº“...")
    vectorstore = Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings
    )

    # 4. æ£€æŸ¥æ•°æ®
    count = vectorstore._collection.count()
    logger.info(f"å‘é‡åº“ä¸­å…±æœ‰ {count} ä¸ªç‰‡æ®µ")
    print(f"ğŸ“š å·²ç´¢å¼• {count} ä¸ªçŸ¥è¯†ç‰‡æ®µ\n")

    # 5. æ£€ç´¢ç›¸å…³å†…å®¹
    print("ğŸ” æ£€ç´¢ç›¸å…³çŸ¥è¯†...\n")
    docs = vectorstore.similarity_search(question, k=3)

    # æ”¶é›†æ¥æºåˆ—è¡¨ï¼ˆå»é‡ï¼‰
    sources = []
    seen = set()
    for doc in docs:
        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
        if source not in seen:
            seen.add(source)
            sources.append(source)

    print("ã€æ£€ç´¢ç»“æœã€‘")
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
        print(f"\n--- ç»“æœ {i} (æ¥æº: {source}) ---")
        # æ˜¾ç¤ºå‰200å­—
        content = doc.page_content[:200].replace('\n', ' ')
        print(f"{content}...")

    # 6. ç”Ÿæˆå›ç­”
    print("\n" + "=" * 60)
    print("ğŸ¤– KAI å›ç­”ï¼š")
    print("=" * 60)

    qa_chain = create_qa_chain(vectorstore)
    result = qa_chain.invoke({"query": question})

    answer = result["result"]
    print(answer)

    # 7. ä¿å­˜åˆ°æ–‡ä»¶
    filepath = save_output(question, answer, sources)
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {filepath}")


if __name__ == "__main__":
    main()
