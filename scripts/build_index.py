#!/usr/bin/env python3
"""
å‘é‡åŒ–è„šæœ¬ï¼šå°† knowledge_base ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° Chroma

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 build_index.py

ä¾èµ–ï¼š
    - langchain
    - langchain-community
    - chromadb
    - langchain-openai (ç”¨äº OpenAI å…¼å®¹çš„ embedding æ¥å£)
"""

import os
import glob
import logging
from pathlib import Path

from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader, UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# OpenAIEmbeddings å·²ä¸å†éœ€è¦ï¼Œä½¿ç”¨æœ¬åœ° HuggingFace æ¨¡å‹
from langchain_community.vectorstores import Chroma

# ========== é…ç½® ==========
KNOWLEDGE_BASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "knowledge_base")
PERSIST_DIR = "./chroma_db_data"

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ========== æ—¥å¿—é…ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def get_embedding_model():
    """
    è·å– embedding æ¨¡å‹

    ä¼˜å…ˆä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡æ¨¡å‹(shibing624/text2vec-base-chinese)ï¼Œ
    é¿å… API è°ƒç”¨å¤±è´¥çš„é—®é¢˜ã€‚
    """
    try:
        from langchain_huggingface import HuggingFaceEmbeddings

        logger.info("ä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = HuggingFaceEmbeddings(
            model_name="shibing624/text2vec-base-chinese",
            model_kwargs={'device': 'cpu'}
        )
        return embeddings
    except ImportError:
        # Fallback to sentence-transformers
        from langchain_community.embeddings import SentenceTransformerEmbeddings

        logger.info("ä½¿ç”¨ SentenceTransformer ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = SentenceTransformerEmbeddings(
            model_name="shibing624/text2vec-base-chinese"
        )
        return embeddings


def load_markdown_files(base_dir):
    """
    åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ .md æ–‡ä»¶
    """
    md_files = glob.glob(os.path.join(base_dir, "*.md"))
    logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ª .md æ–‡ä»¶")

    documents = []

    for file_path in md_files:
        filename = os.path.basename(file_path)
        try:
            # ä½¿ç”¨ TextLoader åŠ è½½ markdown
            loader = TextLoader(file_path, encoding="utf-8")
            docs = loader.load()

            # æ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
            for doc in docs:
                doc.metadata = {
                    "source": filename,
                    "filepath": file_path,
                    "filename": filename
                }

            documents.extend(docs)
            logger.info(f"  âœ“ åŠ è½½: {filename}")

        except Exception as e:
            logger.warning(f"  âœ— åŠ è½½å¤±è´¥ {filename}: {e}")

    logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def split_documents(documents):
    """
    ä½¿ç”¨ RecursiveCharacterTextSplitter åˆ‡åˆ†æ–‡æ¡£

    ä¼˜å…ˆæŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
    """
    # åˆ†éš”ç¬¦ä¼˜å…ˆçº§ï¼šMarkdown æ ‡é¢˜ > æ®µè½ > å¥å­ > å•è¯
    separators = [
        "\n## ",      # äºŒçº§æ ‡é¢˜
        "\n### ",     # ä¸‰çº§æ ‡é¢˜
        "\n",         # æ®µè½
        " ",          # å¥å­è¾¹ç•Œ
        ""            # å•è¯è¾¹ç•Œ
    ]

    splitter = RecursiveCharacterTextSplitter(
        separators=separators,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        add_start_index=True
    )

    chunks = splitter.split_documents(documents)

    logger.info(f"åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
    return chunks


def create_vector_store(chunks, embeddings):
    """
    åˆ›å»º Chroma å‘é‡æ•°æ®åº“å¹¶æŒä¹…åŒ–
    """
    # å¦‚æœå·²å­˜åœ¨æ•°æ®åº“ï¼Œå…ˆåˆ é™¤
    if os.path.exists(PERSIST_DIR):
        logger.info(f"å‘ç°å·²å­˜åœ¨çš„æ•°æ®åº“ï¼Œå°†è¦†ç›–æ›´æ–°...")

    logger.info("æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")

    # åˆ›å»º Chroma å‘é‡åº“
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=PERSIST_DIR
    )

    # ç¡®ä¿æ•°æ®æŒä¹…åŒ–
    vectorstore.persist()

    logger.info(f"âœ“ å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {PERSIST_DIR}")
    return vectorstore


def main():
    """
    ä¸»æµç¨‹ï¼šåŠ è½½ -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å‚¨
    """
    print("=" * 60)
    print("KAI çŸ¥è¯†åº“å‘é‡åŒ–è„šæœ¬")
    print("=" * 60)
    print()

    # 1. æ£€æŸ¥ç›®å½•
    if not os.path.exists(KNOWLEDGE_BASE_DIR):
        logger.error(f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {KNOWLEDGE_BASE_DIR}")
        return

    # 2. åŠ è½½æ–‡æ¡£
    print(f"ğŸ“‚ åŠ è½½æ–‡æ¡£: {KNOWLEDGE_BASE_DIR}")
    documents = load_markdown_files(KNOWLEDGE_BASE_DIR)
    print()

    if not documents:
        logger.warning("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
        return

    # 3. åˆ‡åˆ†æ–‡æ¡£
    print("ğŸ”ª åˆ‡åˆ†æ–‡æ¡£...")
    chunks = split_documents(documents)
    print()

    # 4. åˆå§‹åŒ– embedding æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ– embedding æ¨¡å‹...")
    try:
        embeddings = get_embedding_model()
        # æµ‹è¯• embedding
        test_vector = embeddings.embed_query("æµ‹è¯•")
        logger.info(f"Embedding ç»´åº¦: {len(test_vector)}")
        print()
    except Exception as e:
        logger.error(f"åˆå§‹åŒ– embedding å¤±è´¥: {e}")
        return

    # 5. åˆ›å»ºå‘é‡åº“
    print("ğŸ’¾ åˆ›å»ºå‘é‡æ•°æ®åº“...")
    vectorstore = create_vector_store(chunks, embeddings)
    print()

    # 6. ç»Ÿè®¡ä¿¡æ¯
    print("=" * 60)
    print("âœ… æˆåŠŸç´¢å¼•äº† {} ä¸ªç‰‡æ®µ".format(len(chunks)))
    print(f"ğŸ“ å‘é‡åº“ä½ç½®: {os.path.abspath(PERSIST_DIR)}")
    print("=" * 60)


if __name__ == "__main__":
    main()
