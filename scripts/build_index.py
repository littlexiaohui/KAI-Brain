#!/usr/bin/env python3
"""
å‘é‡åŒ–è„šæœ¬ï¼šå°† knowledge_base ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° Chroma

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 scripts/build_index.py

ä¾èµ–ï¼š
    - langchain
    - langchain-community
    - chromadb
    - langchain-openai (ç”¨äº OpenAI å…¼å®¹çš„ embedding æ¥å£)
    - python-frontmatter (V5.1 ç”¨äºè§£æ Frontmatter)

åˆ‡åˆ†ç­–ç•¥ (V3.3):
    - ç¬¬ä¸€å±‚ï¼šæŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†ï¼ˆä¿è¯è¯­ä¹‰å®Œæ•´æ€§ï¼‰
    - ç¬¬äºŒå±‚ï¼šé€’å½’ç»†åˆ‡ï¼ˆé˜²æ­¢å•ç« è¿‡é•¿ï¼‰

å…ƒæ•°æ® (V5.1 å››å¤§é‡‘åˆš):
    - source: æ¥æºå¹³å° (xiaohongshu/wechat/douyin)
    - created_at: åˆ›å»ºæ—¥æœŸ
    - author: ä½œè€…
    - content_type: å†…å®¹ç±»å‹ (post/article/script/doc)
"""

import os
import glob
import logging
from pathlib import Path

from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader, UnstructuredMarkdownLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# ========== é…ç½® ==========
# çŸ¥è¯†åº“ç›®å½•ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
KNOWLEDGE_BASE_DIR = os.path.join(PROJECT_ROOT, "knowledge_base")
PERSIST_DIR = os.path.join(PROJECT_ROOT, "chroma_db_data")

# åˆ‡åˆ†å‚æ•° (V3.3 æ··åˆåˆ‡åˆ†ç­–ç•¥)
CHUNK_SIZE = 500       # æ¯ä¸ªå—çº¦ 300-500 ä¸­æ–‡å­—
CHUNK_OVERLAP = 50     # é‡å  50 å­—ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±

# V5.1 Frontmatter å››å¤§é‡‘åˆšå­—æ®µ
FRONTMATTER_FIELDS = ['source', 'created_at', 'author', 'content_type']

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ========== æ—¥å¿—é…ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def get_embedding_model():
    """
    è·å– embedding æ¨¡å‹

    ä¼˜å…ˆä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡æ¨¡å‹(shibing624/text2vec-base-chinese)ï¼Œ
    é¿å… API è°ƒç”¨å¤±è´¥çš„é—®é¢˜ã€‚
    """
    try:
        from langchain_huggingface import HuggingFaceEmbeddings

        logger.info("ä½¿ç”¨æœ¬åœ° HuggingFace ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = HuggingFaceEmbeddings(
            model_name="shibing624/text2vec-base-chinese",
            model_kwargs={'device': 'cpu'}
        )
        return embeddings
    except ImportError:
        # Fallback to sentence-transformers
        from langchain_community.embeddings import SentenceTransformerEmbeddings

        logger.info("ä½¿ç”¨ SentenceTransformer ä¸­æ–‡ embedding æ¨¡å‹: shibing624/text2vec-base-chinese")
        embeddings = SentenceTransformerEmbeddings(
            model_name="shibing624/text2vec-base-chinese"
        )
        return embeddings


def load_markdown_files(base_dir):
    """
    é€’å½’åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ .md æ–‡ä»¶
    V5.1 æ–°å¢ï¼šè§£æ YAML Frontmatter å››å¤§é‡‘åˆšå­—æ®µ
    """
    import frontmatter

    # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
    md_files = glob.glob(os.path.join(base_dir, "**/*.md"), recursive=True)
    logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ª .md æ–‡ä»¶")

    documents = []

    for file_path in md_files:
        filename = os.path.basename(file_path)
        try:
            # V5.1 ä½¿ç”¨ python-frontmatter è§£æ Frontmatter
            with open(file_path, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)

            # è·å– Frontmatter å…ƒæ•°æ®ï¼ˆå››å¤§é‡‘åˆšï¼‰
            fm_metadata = post.metadata

            # åŸºç¡€å…ƒæ•°æ®
            metadata = {
                "source": fm_metadata.get('source', 'unknown'),
                "filepath": file_path,
                "filename": filename,
                "created_at": fm_metadata.get('created_at', ''),
                "author": fm_metadata.get('author', 'KAI'),
                "content_type": fm_metadata.get('content_type', 'note')
            }

            # åˆ›å»º Document
            from langchain.schema import Document
            doc = Document(page_content=post.content, metadata=metadata)

            documents.append(doc)
            logger.info(f"  âœ“ åŠ è½½: {filename} [{metadata['source']}]")

        except Exception as e:
            logger.warning(f"  âœ— åŠ è½½å¤±è´¥ {filename}: {e}")

    logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def split_documents(documents):
    """
    V3.3 æ··åˆåˆ‡åˆ†ç­–ç•¥ï¼šMarkdownHeaderTextSplitter + RecursiveCharacterTextSplitter

    ç¬¬ä¸€å±‚ï¼šæŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†ï¼ˆä¿è¯è¯­ä¹‰å®Œæ•´æ€§ï¼‰
    ç¬¬äºŒå±‚ï¼šé€’å½’ç»†åˆ‡ï¼ˆé˜²æ­¢å•ç« è¿‡é•¿ï¼‰
    """
    # 1. ç¬¬ä¸€å±‚ï¼šæŒ‰æ ‡é¢˜åˆ‡åˆ†ï¼ˆä¿ç•™å±‚çº§å…ƒæ•°æ®ï¼‰
    headers_to_split_on = [
        ("#", "Header 1"),      # ä¸€çº§æ ‡é¢˜
        ("##", "Header 2"),     # äºŒçº§æ ‡é¢˜
        ("###", "Header 3"),    # ä¸‰çº§æ ‡é¢˜
    ]

    # å…ˆæŒ‰æ ‡é¢˜åˆ‡åˆ†
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on
    )
    header_splits = []
    for doc in documents:
        splits = markdown_splitter.split_text(doc.page_content)
        for split in splits:
            # ä¿ç•™åŸæœ‰å…ƒæ•°æ® + æ·»åŠ æ ‡é¢˜å…ƒæ•°æ®
            split.metadata = {**doc.metadata, **split.metadata}
            split.metadata['filepath'] = doc.metadata.get('filepath', '')
            header_splits.append(split)

    logger.info(f"ç¬¬ä¸€å±‚æŒ‰æ ‡é¢˜åˆ‡åˆ†: {len(header_splits)} ä¸ªç‰‡æ®µ")

    # 2. ç¬¬äºŒå±‚ï¼šé€’å½’ç»†åˆ‡ï¼ˆé˜²æ­¢å•ç« è¿‡é•¿ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " "],  # ä¼˜å…ˆæŒ‰æ®µè½å’Œå¥å­åˆ‡
        length_function=len,
        add_start_index=True
    )

    chunks = text_splitter.split_documents(header_splits)

    logger.info(f"ç¬¬äºŒå±‚é€’å½’ç»†åˆ‡å: {len(chunks)} ä¸ªç‰‡æ®µ")
    return chunks


def create_vector_store(chunks, embeddings):
    """
    åˆ›å»º Chroma å‘é‡æ•°æ®åº“å¹¶æŒä¹…åŒ–
    """
    # å¦‚æœå·²å­˜åœ¨æ•°æ®åº“ï¼Œå…ˆåˆ é™¤
    if os.path.exists(PERSIST_DIR):
        logger.info(f"å‘ç°å·²å­˜åœ¨çš„æ•°æ®åº“ï¼Œå°†è¦†ç›–æ›´æ–°...")

    logger.info("æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")

    # åˆ›å»º Chroma å‘é‡åº“
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=PERSIST_DIR
    )

    # ç¡®ä¿æ•°æ®æŒä¹…åŒ–
    vectorstore.persist()

    logger.info(f"âœ“ å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {PERSIST_DIR}")
    return vectorstore


def main():
    """
    ä¸»æµç¨‹ï¼šåŠ è½½ -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å‚¨
    """
    print("=" * 60)
    print("KAI çŸ¥è¯†åº“å‘é‡åŒ–è„šæœ¬")
    print("=" * 60)
    print()

    # 1. æ£€æŸ¥ç›®å½•
    if not os.path.exists(KNOWLEDGE_BASE_DIR):
        logger.error(f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {KNOWLEDGE_BASE_DIR}")
        return

    # 2. åŠ è½½æ–‡æ¡£
    print(f"ğŸ“‚ åŠ è½½æ–‡æ¡£: {KNOWLEDGE_BASE_DIR}")
    documents = load_markdown_files(KNOWLEDGE_BASE_DIR)
    print()

    if not documents:
        logger.warning("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
        return

    # 3. åˆ‡åˆ†æ–‡æ¡£
    print("ğŸ”ª åˆ‡åˆ†æ–‡æ¡£...")
    chunks = split_documents(documents)
    print()

    # 4. åˆå§‹åŒ– embedding æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ– embedding æ¨¡å‹...")
    try:
        embeddings = get_embedding_model()
        # æµ‹è¯• embedding
        test_vector = embeddings.embed_query("æµ‹è¯•")
        logger.info(f"Embedding ç»´åº¦: {len(test_vector)}")
        print()
    except Exception as e:
        logger.error(f"åˆå§‹åŒ– embedding å¤±è´¥: {e}")
        return

    # 5. åˆ›å»ºå‘é‡åº“
    print("ğŸ’¾ åˆ›å»ºå‘é‡æ•°æ®åº“...")
    vectorstore = create_vector_store(chunks, embeddings)
    print()

    # 6. ç»Ÿè®¡ä¿¡æ¯
    print("=" * 60)
    print("âœ… æˆåŠŸç´¢å¼•äº† {} ä¸ªç‰‡æ®µ".format(len(chunks)))
    print(f"ğŸ“ å‘é‡åº“ä½ç½®: {os.path.abspath(PERSIST_DIR)}")
    print("=" * 60)


if __name__ == "__main__":
    main()
